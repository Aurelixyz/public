import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from email.header import Header
from email.mime.image import MIMEImage
import configparser
import pyodbc
import pandas as pd
from datetime import datetime, timedelta
import codecs
import matplotlib.pyplot as plt
import seaborn as sns
import io
import numpy as np
from matplotlib.colors import LinearSegmentedColormap
import base64
import logging

# === Konfiguracja Logowania ===
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("raport_generator.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)

# === Krok 1: Wczytaj dane konfiguracyjne ===
logging.info("Krok 1: Wczytywanie konfiguracji...")
config = configparser.ConfigParser()
try:
    with codecs.open('config.ini', 'r', encoding='utf-8') as f:
        config.read_file(f)
except FileNotFoundError:
    logging.error("Plik config.ini nie został znaleziony.")
    exit()
except Exception:
    logging.exception("Wystąpił krytyczny błąd podczas wczytywania pliku config.ini.")
    exit()

# Odczyt ustawień
sender_email = config.get('EMAIL', 'Sender')
receiver_email = config.get('EMAIL', 'Receiver')
smtp_server = config.get('EMAIL', 'SMTPServer')
subject = config.get('EMAIL', 'Subject')
conn_str = config.get('DATABASE', 'ConnectionString')
generate_plots = config.getboolean('REPORT_SETTINGS', 'GeneratePlots', fallback=True)
TOTAL_LIMIT = config.getint('REPORT_SETTINGS', 'TotalConsultantLimit', fallback=18)
DEPT_LIMIT = config.getint('REPORT_SETTINGS', 'DepartmentConsultantLimit', fallback=3)
limit_rbh = config.getfloat('REPORT_SETTINGS', 'MonthlyLimitRBH', fallback=1500.0)
logging.info("Konfiguracja wczytana pomyślnie.")

# === Krok 2: Wczytaj zapytania SQL z plików ===
logging.info("Krok 2: Wczytywanie zapytań SQL...")
try:
    with open("zapytanieREPLIKA.sql", "r", encoding="utf-8") as file:
        query_aktywnosci = file.read()
    with open("zapytanieListaKoordynatorow.sql", "r", encoding="utf-8") as file:
        query_koordynatorzy = file.read()
    logging.info("Zapytania SQL wczytane.")
except FileNotFoundError as e:
    logging.error(f"Plik SQL nie został znaleziony: {e.filename}")
    exit()
except Exception:
    logging.exception("Wystąpił błąd podczas wczytywania plików SQL.")
    exit()

# === Krok 3: Połącz się z bazą danych i pobierz dane ===
logging.info("Krok 3: Pobieranie danych z bazy...")
try:
    conn = pyodbc.connect(conn_str)
    df_aktywnosci = pd.read_sql(query_aktywnosci, conn)
    df_koordynatorzy = pd.read_sql(query_koordynatorzy, conn)
    conn.close()
    logging.info(f"Pobrano {len(df_aktywnosci)} rekordów aktywności.")
    logging.info(f"Pobrano {len(df_koordynatorzy)} wszystkich przełożonych (przed filtrowaniem).")
except pyodbc.Error as ex:
    sqlstate = ex.args[0]
    logging.error(f"Błąd połączenia z bazą danych lub wykonania zapytania: {sqlstate}")
    exit()
except Exception:
    logging.exception("Wystąpił nieoczekiwany błąd podczas operacji na bazie danych.")
    exit()

# === Krok 4: Przetwarzanie i czyszczenie danych ===
logging.info("Krok 4: Przetwarzanie i czyszczenie danych...")

# Ujednolicenie nazwy kolumny-klucza (loginu koordynatora)
logging.info(" -> Ujednolicanie nazwy kolumny kluczowej...")
df_koordynatorzy.rename(columns={'LoginWindows': 'PrzelozonyLoginWindows'}, inplace=True)
logging.info("Nazwa kolumny koordynatora ujednolicona.")

# Filtrowanie tylko koordynatorów
if 'StanowiskoTaryfikatoroweNazwa' in df_koordynatorzy.columns:
    logging.info(" -> Filtrowanie listy, aby uwzględnić tylko koordynatorów...")
    df_koordynatorzy = df_koordynatorzy[df_koordynatorzy['StanowiskoTaryfikatoroweNazwa'] == 'Koordynator'].copy()
    logging.info(f"Lista przefiltrowana. Pozostało {len(df_koordynatorzy)} koordynatorów.")
else:
    logging.warning(
        "Brak kolumny 'StanowiskoTaryfikatoroweNazwa' w zapytaniu o listę przełożonych. Analiza obejmie wszystkich.")

# Filtrowanie ról, aby analizować tylko konsultantów
if 'RolaNazwa' in df_aktywnosci.columns:
    logging.info(" -> Filtrowanie ról konsultantów...")
    role_konsultantow = ['Agent', 'Pre - Ekspert', 'Ekspert']
    df_aktywnosci = df_aktywnosci[df_aktywnosci['RolaNazwa'].isin(role_konsultantow)]
    logging.info(f"Dane przefiltrowane. Pozostało {len(df_aktywnosci)} rekordów.")

# Konwersja typów dat i obsługa braku danych
if df_aktywnosci.empty:
    logging.warning("Brak danych o aktywnościach do przetworzenia. Raport może być niekompletny.")
    df = pd.DataFrame(
        columns=['StatusDateTime', 'EndDateTime', 'UserId', 'Wydzial', 'PrzelozonyLoginWindows', 'RolaNazwa'])
else:
    df = df_aktywnosci.copy()
    df['StatusDateTime'] = pd.to_datetime(df['StatusDateTime'])
    df['EndDateTime'] = pd.to_datetime(df['EndDateTime'])


# Funkcja dzieląca sesje na godzinowe interwały
def calculate_seconds(row):
    start = row['StatusDateTime']
    end = row['EndDateTime']
    intervals = []
    while start < end:
        next_hour = (start + timedelta(hours=1)).replace(minute=0, second=0, microsecond=0)
        if next_hour > end:
            next_hour = end
        seconds = (next_hour - start).total_seconds()
        # Przekazujemy login przełożonego i rolę do dalszej analizy
        intervals.append(
            (start.hour, seconds, row['UserId'], row['Wydzial'], row['PrzelozonyLoginWindows'], row['RolaNazwa']))
        start = next_hour
    return intervals


# Przetwarzanie interwałów
logging.info(" -> Obliczanie interwałów godzinowych...")
intervals = []
if not df.empty:
    for _, row in df.iterrows():
        intervals.extend(calculate_seconds(row))

intervals_df = pd.DataFrame(intervals,
                            columns=['Hour', 'Seconds', 'UserId', 'Wydzial', 'PrzelozonyLoginWindows', 'RolaNazwa'])

# --- POCZĄTEK ORYGINALNEJ LOGIKI PRZETWARZANIA DLA GŁÓWNEGO RAPORTU ---
# Ta sekcja jest w całości zachowana z Twojego oryginalnego kodu

grouped = intervals_df.groupby('Hour').agg({'Seconds': 'sum'}).reset_index()
unique_users = intervals_df.groupby('Hour')['UserId'].nunique().reset_index()
unique_users.columns = ['Hour', 'UniqueConsultants']
all_hours = pd.DataFrame({'Hour': range(24)})
grouped = pd.merge(all_hours, grouped, on='Hour', how='left')
grouped = pd.merge(grouped, unique_users, on='Hour', how='left').fillna(0)


def seconds_to_minutes(seconds):
    return round(seconds / 60)


grouped['Seconds'] = grouped['Seconds'].apply(seconds_to_minutes)
departments = ['WZK1', 'WZK2', 'WZK3', 'WZF', 'WNT', 'WZP']
for dept in departments:
    dept_seconds = intervals_df[intervals_df['Wydzial'] == dept].groupby('Hour')['Seconds'].sum().reset_index()
    dept_seconds.columns = ['Hour', dept]
    grouped = pd.merge(grouped, dept_seconds, on='Hour', how='left').fillna(0)
    grouped[dept] = grouped[dept].apply(seconds_to_minutes)


def max_simultaneous_consultants(df_current_hour, hour):
    base_date = df_current_hour[
        'StatusDateTime'].dt.normalize().min() if not df_current_hour.empty else datetime.now().normalize()
    hour_start = base_date + pd.Timedelta(hours=hour)
    hour_end = hour_start + pd.Timedelta(hours=1)
    df_hour = df_current_hour[
        (df_current_hour['StatusDateTime'] < hour_end) & (df_current_hour['EndDateTime'] > hour_start)]
    timeline = []
    for _, row in df_hour.iterrows():
        start = max(row['StatusDateTime'], hour_start)
        end = min(row['EndDateTime'], hour_end)
        timeline.append((start, 1))
        timeline.append((end, -1))
    timeline.sort()
    active = 0;
    interval_start = None;
    interval_active = 0;
    valid_intervals = []
    over_total_limit_seconds = [0.0] * 6
    if not timeline:
        return 0, pd.NaT, pd.NaT, *over_total_limit_seconds

    current_time = hour_start
    for time, change in timeline:
        if time > current_time and interval_active is not None:
            duration = (time - current_time).total_seconds()
            for i in range(6):
                if interval_active >= (TOTAL_LIMIT + 1 + i):
                    over_total_limit_seconds[i] += duration
            if duration >= 60 and interval_active >= 2:
                valid_intervals.append((current_time, time, interval_active))

        active += change
        current_time = time
        interval_active = active

    if current_time < hour_end:
        duration = (hour_end - current_time).total_seconds()
        for i in range(6):
            if interval_active >= (TOTAL_LIMIT + 1 + i):
                over_total_limit_seconds[i] += duration
        if duration >= 60 and interval_active >= 2:
            valid_intervals.append((current_time, hour_end, interval_active))

    if valid_intervals:
        max_interval = max(valid_intervals, key=lambda x: x[2])
        return (max_interval[2], max_interval[0], max_interval[1], *over_total_limit_seconds)
    else:
        max_active = 0
        active = 0
        for time, change in timeline:
            active += change
            if active > max_active:
                max_active = active
        return (max_active, pd.NaT, pd.NaT, *over_total_limit_seconds)


def time_over_dept_limit(df_current_hour, hour, department):
    base_date = df_current_hour[
        'StatusDateTime'].dt.normalize().min() if not df_current_hour.empty else datetime.now().normalize()
    hour_start = base_date + pd.Timedelta(hours=hour)
    hour_end = hour_start + pd.Timedelta(hours=1)
    df_hour_dept = df_current_hour[
        (df_current_hour['StatusDateTime'] < hour_end) & (df_current_hour['EndDateTime'] > hour_start) & (
            df_current_hour['Wydzial'] == department)]
    if df_hour_dept.empty:
        return 0.0

    timeline = []
    for _, row in df_hour_dept.iterrows():
        start = max(row['StatusDateTime'], hour_start)
        end = min(row['EndDateTime'], hour_end)
        timeline.append((start, 1))
        timeline.append((end, -1))
    timeline.sort()

    active = 0;
    over_limit_time = 0.0
    current_time = hour_start
    for time, change in timeline:
        if time > current_time:
            duration = (time - current_time).total_seconds()
            if active > DEPT_LIMIT:
                over_limit_time += duration

        active += change
        current_time = time

    if current_time < hour_end:
        duration = (hour_end - current_time).total_seconds()
        if active > DEPT_LIMIT:
            over_limit_time += duration

    return over_limit_time


total_cols = [f'Over{TOTAL_LIMIT + 1 + i}ConsultantsMinutes' for i in range(6)]
results = grouped['Hour'].apply(lambda h: max_simultaneous_consultants(df, h))
result_cols = ['MaxConsultants', 'MaxConsultantsStart', 'MaxConsultantsEnd'] + total_cols
temp_df = pd.DataFrame(results.tolist(), columns=result_cols, index=grouped.index)
grouped = pd.concat([grouped, temp_df], axis=1)

for dept in departments:
    grouped[f'{dept}_Over{DEPT_LIMIT}ConsultantsMinutes'] = grouped['Hour'].apply(
        lambda h: time_over_dept_limit(df, h, dept))

cols_to_convert = total_cols + [f'{dept}_Over{DEPT_LIMIT}ConsultantsMinutes' for dept in departments]
for col in cols_to_convert:
    if col in grouped.columns:
        grouped[col] = grouped[col].apply(seconds_to_minutes)

logging.info("Dane dla raportu głównego przetworzone w całości.")
# --- KONIEC ORYGINALNEJ LOGIKI PRZETWARZANIA ---


# === Krok 5: Zapisanie danych do pliku CSV ===
report_date_str_for_filename = df['StatusDateTime'].min().normalize().strftime(
    "%Y-%m-%d") if not df.empty else datetime.now().strftime("%Y-%m-%d")
csv_filename = f"raport_dzienny_{report_date_str_for_filename}.csv"
grouped.to_csv(csv_filename, index=False, encoding='utf-8-sig')
logging.info(f"Krok 5: Dane raportu zostały zapisane do pliku: {csv_filename}")


# === NOWA, UNIWERSALNA FUNKCJA DO TWORZENIA DASHBOARDÓW ===
def stworz_dashboard(heatmap_data, fig_title, main_title, y_axis_label, right_bar_title, bottom_bar_title, cmap,
                     cbar_label='Minuty'):
    """
    Tworzy i zapisuje do bufora spójny wizualnie dashboard.
    """
    fig_height = max(10, len(heatmap_data.index) * 1.0)
    fig = plt.figure(figsize=(18, fig_height))

    # Główna mapa ciepła
    ax_main = plt.subplot2grid((5, 5), (0, 0), rowspan=3, colspan=4)
    im = ax_main.imshow(heatmap_data.values, cmap=cmap, aspect='auto', interpolation='nearest', vmin=0)
    ax_main.set_xticks(np.arange(heatmap_data.shape[1]))
    ax_main.set_xticklabels([f"{h:02d}:00" for h in heatmap_data.columns], rotation=45, ha="right")
    ax_main.set_yticks(np.arange(len(heatmap_data.index)))
    ax_main.set_yticklabels(heatmap_data.index)
    ax_main.set_title(main_title, fontsize=12)
    ax_main.set_ylabel(y_axis_label, fontsize=12)

    # Dodawanie wartości do komórek heatmapy
    for r in range(len(heatmap_data.index)):
        for c in range(heatmap_data.shape[1]):
            value = heatmap_data.iloc[r, c]
            if value > 0:
                text_color = 'white' if value > np.nanmax(heatmap_data.values) * 0.5 else 'black'
                ax_main.text(c, r, f'{int(value)}', ha='center', va='center', color=text_color, fontsize=9)

    # Boczny wykres słupkowy
    ax_right = plt.subplot2grid((5, 5), (0, 4), rowspan=3, colspan=1)
    sumy_per_row = heatmap_data.sum(axis=1)
    bar_colors = plt.cm.Set3(np.linspace(0, 1, len(sumy_per_row)))
    bars = ax_right.barh(sumy_per_row.index, sumy_per_row.values, color=bar_colors, alpha=0.8)
    ax_right.set_title(right_bar_title, fontsize=12)
    ax_right.tick_params(axis='y', length=0, labelsize=9)
    ax_right.invert_yaxis()

    # Dodawanie etykiet do słupków bocznych
    for bar in bars:
        width = bar.get_width()
        if width > 0:
            x_pos = width - (ax_right.get_xlim()[1] * 0.02)
            y_pos = bar.get_y() + bar.get_height() / 2
            ax_right.text(x_pos, y_pos, f'{int(width)}', va='center', ha='right', color='white', fontsize=9)

    # Dolny wykres słupkowy
    ax_bottom = plt.subplot2grid((5, 5), (3, 0), rowspan=2, colspan=4)
    sumy_per_col = heatmap_data.sum(axis=0)
    ax_bottom.bar(sumy_per_col.index, sumy_per_col.values, color='steelblue', alpha=0.7)
    ax_bottom.set_title(bottom_bar_title, fontsize=12)
    ax_bottom.set_xticks(np.arange(0, 24, 2))
    ax_bottom.set_xlim(-0.5, 23.5)

    # Colorbar
    cbar = plt.colorbar(im, ax=ax_main, shrink=0.8, aspect=20);
    cbar.set_label(cbar_label, fontsize=11)

    fig.suptitle(fig_title, fontsize=18)
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])

    img_buffer = io.BytesIO()
    plt.savefig(img_buffer, format='png', dpi=200, bbox_inches='tight', facecolor='white')
    img_buffer.seek(0)
    plt.close(fig)
    return img_buffer


# === Krok 6: Generowanie wizualizacji ===
image_buffers = []
if generate_plots:
    logging.info("Krok 6: Generowanie wizualizacji...")
    plt.rcParams['font.family'] = 'sans-serif'
    plt.rcParams['font.sans-serif'] = ['Arial']

    # --- WYKRES 1: Suma minut przekroczenia (Zmodyfikowany) ---
    logging.info(" -> Generowanie wykresu 1...")
    fig = plt.figure(figsize=(24, 10)) # Zwiększono szerokość wykresu
    ax = fig.add_subplot(111)
    fig.subplots_adjust(left=0.15, right=0.85, top=0.85, bottom=0.2)
    heatmap_data_przekroczenia = grouped[total_cols].T
    heatmap_data_przekroczenia.columns = [f"{h:02d}:00" for h in grouped['Hour']]
    heatmap_data_przekroczenia.index = [f'≥{TOTAL_LIMIT + 1 + i} osób' for i in range(6)]
    colors_red = ['#ffffff', '#ffe6e6', '#ffcccc', '#ff9999', '#ff6666', '#ff3333', '#cc0000']
    cmap_red = LinearSegmentedColormap.from_list('custom_red', colors_red, N=100)
    im = ax.imshow(heatmap_data_przekroczenia.values, cmap=cmap_red, aspect='auto', interpolation='nearest', vmin=0)
    
    # Dodanie siatki
    ax.set_xticks(np.arange(heatmap_data_przekroczenia.shape[1] + 1) - .5, minor=True)
    ax.set_yticks(np.arange(heatmap_data_przekroczenia.shape[0] + 1) - .5, minor=True)
    ax.grid(which="minor", color="gray", linestyle='-', linewidth=0.5)
    ax.tick_params(which="minor", bottom=False, left=False)

    ax.set_xlim(-0.5, len(heatmap_data_przekroczenia.columns) - 0.5);
    ax.set_ylim(-0.5, len(heatmap_data_przekroczenia.index) - 0.5)
    ax.set_xticks(range(0, len(heatmap_data_przekroczenia.columns), 2));
    ax.set_xticklabels(
        [heatmap_data_przekroczenia.columns[i] for i in range(0, len(heatmap_data_przekroczenia.columns), 2)],
        fontsize=10, rotation=45, ha="right", weight='bold') # Pogrubienie etykiet X
    ax.set_yticks(range(len(heatmap_data_przekroczenia.index)));
    ax.set_yticklabels(heatmap_data_przekroczenia.index, fontsize=11, weight='bold') # Pogrubienie etykiet Y
    for i in range(len(heatmap_data_przekroczenia.index)):
        for j in range(len(heatmap_data_przekroczenia.columns)):
            value = heatmap_data_przekroczenia.iloc[i, j]
            if value > 0:
                text_color = 'white' if value > heatmap_data_przekroczenia.values.max() * 0.6 else 'black'
                ax.text(j, i, f'{int(value)}', ha='center', va='center', fontsize=9, color=text_color, weight='bold') # Pogrubienie wartości
    cbar = plt.colorbar(im, ax=ax, shrink=0.8, aspect=30, pad=0.02);
    cbar.set_label('Minuty przekroczenia', fontsize=11, weight='bold'); # Pogrubienie etykiety colorbar
    cbar.ax.tick_params(labelsize=10)
    ax.set_title('Suma minut przekroczenia w danym interwale', fontsize=18, pad=20)
    ax.set_xlabel('Godzina', fontsize=12, weight='bold'); # Pogrubienie etykiety X
    ax.set_ylabel('Próg liczby konsultantów', fontsize=12, weight='bold') # Pogrubienie etykiety Y
    img_buffer_1 = io.BytesIO()
    plt.savefig(img_buffer_1, format='png', dpi=300, bbox_inches='tight', facecolor='white')
    img_buffer_1.seek(0)
    image_buffers.append(img_buffer_1)
    plt.close()

    # Definicja wspólnego stylu dla dashboardu naruszeń wydziałowych
    colors_blue = ['#f7fbff', '#deebf7', '#c6dbef', '#9ecae1', '#6baed6', '#4292c6', '#2171b5', '#084594']
    cmap_blue = LinearSegmentedColormap.from_list('custom_blue', colors_blue, N=100)

    # --- WYKRES 2: Dashboard naruszeń wydziałowych (użycie nowej funkcji) ---
    logging.info(" -> Generowanie dashboardu naruszeń wydziałowych...")
    dept_heatmap_cols = [f'{dept}_Over{DEPT_LIMIT}ConsultantsMinutes' for dept in departments]
    df_heatmap_wydzialy = grouped.set_index('Hour')[dept_heatmap_cols].T
    df_heatmap_wydzialy = df_heatmap_wydzialy.reindex(columns=range(24)).fillna(0)
    df_heatmap_wydzialy.index = departments
    img_buffer_2 = stworz_dashboard(
        heatmap_data=df_heatmap_wydzialy,
        fig_title=f'Dashboard naruszeń limitów wydziałowych (minuty z >{DEPT_LIMIT} konsultantami)',
        main_title='Minuty naruszeń na wydział per godzinę',
        y_axis_label='Wydział',
        right_bar_title='Suma minut\nna wydział',
        bottom_bar_title='Suma minut na godzinę (wszystkie wydziały)',
        cmap=cmap_blue
    )
    image_buffers.append(img_buffer_2)

    # === Usunięto generowanie Dashboardów dla Koordynatorów ===
    logging.info("Wizualizacje dla koordynatorów usunięte zgodnie z prośbą.")

    # --- Usunięto NOWE WYKRESY DODATKOWE ---
    logging.info("Dodatkowe wykresy analityczne usunięte zgodnie z prośbą.")

    logging.info("Wszystkie wizualizacje wygenerowane.")
else:
    logging.warning("Generowanie wizualizacji pominięte (zgodnie z konfiguracją).")

# === Krok 7: Budowanie finalnej treści HTML e-maila ===
logging.info("Krok 7: Budowanie treści e-maila...")
total_seconds_today = intervals_df['Seconds'].sum()
used_rbh = total_seconds_today / 3600.0
remaining_rbh = limit_rbh - used_rbh
limit_rbh_formatted = f"{limit_rbh:.2f}";
used_rbh_formatted = f"{used_rbh:.2f}";
remaining_rbh_formatted = f"{remaining_rbh:.2f}"
report_date = df['StatusDateTime'].min().normalize() if not df.empty else datetime.now().normalize()
day_names_polish = {'Monday': 'Poniedziałek', 'Tuesday': 'Wtorek', 'Wednesday': 'Środa', 'Thursday': 'Czwartek',
                    'Friday': 'Piątek', 'Saturday': 'Sobota', 'Sunday': 'Niedziela'}
english_day = report_date.strftime("%A");
polish_day = day_names_polish.get(english_day, english_day)
today_str = report_date.strftime(f"%Y-%m-%d {polish_day}")
header_titles = [("Interwał", 60)]
for i in range(6):
    header_titles.append((f"min. przy ≥{TOTAL_LIMIT + 1 + i} os.", 40))
for dept in departments:
    header_titles.append((f"min. przy >{DEPT_LIMIT} {dept}", 40))

html_base_content = f"""
<!DOCTYPE html>
<html>
<head>
<meta name="color-scheme" content="light dark">
<meta name="supported-color-schemes" content="light dark">
</head>
<body style="background-color:#ffffff; font-family: Arial, sans-serif;">
<p style="font-size:12px; color:#000000;">
Data: {today_str}<br>
Limit w tym miesiącu: {limit_rbh_formatted} RBH<br>
Wykorzystano: {used_rbh_formatted} RBH<br>
Pozostało: {remaining_rbh_formatted} RBH</p>
<table border="1" cellpadding="3" cellspacing="0" style="border-collapse:collapse; background-color:#ffffff; width: 800px;"><tr>
"""
for title, width in header_titles:
    html_base_content += f'<th style="font-family: Arial, sans-serif; font-size:9px; width:{width}px; background-color:#e0e0e0; color:#000000; text-align:center;">{title}</th>'
html_base_content += "</tr>"
for idx, row in grouped.iterrows():
    row_bg_color = "#f2f2f2" if idx % 2 == 0 else "#ffffff"
    html_base_content += "<tr>"
    html_base_content += f'<td style="font-family: Arial, sans-serif; font-size:9px; width:60px; background-color:{row_bg_color}; color:#000000; text-align:center;">{row["Hour"]:02}:00</td>'
    for col in total_cols:
        val = row.get(col, 0);
        display = '' if val == 0 else f'{int(val)}'
        cell_bg_color = '#ffcccc' if val > 0 else row_bg_color
        html_base_content += f'<td style="font-family: Arial, sans-serif; font-size:9px; width:40px; background-color:{cell_bg_color}; color:#000000; text-align:center;">{display}</td>'
    for dept in departments:
        col = f'{dept}_Over{DEPT_LIMIT}ConsultantsMinutes';
        val = row.get(col, 0)
        display = '' if val == 0 else f'{int(val)}';
        cell_bg_color = '#ffcccc' if val > 0 else row_bg_color
        html_base_content += f'<td style="font-family: Arial, sans-serif; font-size:9px; width:40px; background-color:{cell_bg_color}; color:#000000; text-align:center;">{display}</td>'
    html_base_content += "</tr>"
html_base_content += '<tr><td style="font-family: Arial, sans-serif; font-size:9px; width:60px; background-color:#e0e0e0; color:#000000; text-align:center;">Suma</td>'
for col in total_cols + [f'{dept}_Over{DEPT_LIMIT}ConsultantsMinutes' for dept in departments]:
    total_sum = grouped[col].sum() if col in grouped.columns else 0
    display_sum = '' if total_sum == 0 else f'{int(total_sum)}'
    sum_bg_color = '#ffcccc' if total_sum > 0 else '#e0e0e0'
    html_base_content += f'<td style="font-family: Arial, sans-serif; font-size:9px; width:40px; background-color:{sum_bg_color}; color:#000000; text-align:center;">{display_sum}</td>'
html_base_content += "</tr></table>"

final_html_for_email = html_base_content
if generate_plots and image_buffers:
    for i, _ in enumerate(image_buffers):
        final_html_for_email += '<table border="0" cellpadding="0" cellspacing="0" width="100%"><tr><td height="25" style="font-size: 25px; line-height: 25px;">&nbsp;</td></tr></table>'
        final_html_for_email += f'<table border="0" cellpadding="0" cellspacing="0" width="800" align="left"><tr><td><img src="cid:image{i}" width="800" style="width: 800px; height: auto; display: block;"></td></tr></table>'
final_html_for_email += "</body></html>"
logging.info("Treść e-maila gotowa.")

# === Krok 7.5: Zapisz podgląd HTML na dysku ===
logging.info("Krok 7.5: Zapisywanie podglądu HTML na dysku...")
html_filename = f"podglad_raportu_{report_date_str_for_filename}.html"
final_html_for_file = html_base_content
if generate_plots and image_buffers:
    for i, img_buffer in enumerate(image_buffers):
        img_buffer.seek(0)
        base64_image = base64.b64encode(img_buffer.read()).decode('utf-8')
        final_html_for_file += '<table border="0" cellpadding="0" cellspacing="0" width="100%"><tr><td height="25" style="font-size: 25px; line-height: 25px;">&nbsp;</td></tr></table>'
        final_html_for_file += f'<table border="0" cellpadding="0" cellspacing="0" width="800" align="left"><tr><td><img src="data:image/png;base64,{base64_image}" width="800" style="width: 800px; height: auto; display: block;"></td></tr></table>'
final_html_for_file += "</body></html>"
try:
    with open(html_filename, "w", encoding="utf-8") as f:
        f.write(final_html_for_file)
    logging.info(f"Podgląd raportu został pomyślnie zapisany do pliku: {html_filename}")
except Exception:
    logging.exception("Błąd podczas zapisywania podglądu HTML.")

# === Krok 8: Wysyłka e-maila ===
logging.info("Krok 8: Wysyłanie e-maila...")
msg = MIMEMultipart('related')
msg['From'] = sender_email
msg['To'] = receiver_email
msg['Subject'] = Header(subject, 'utf-8')
html_part = MIMEText(final_html_for_email, 'html', 'utf-8')
msg.attach(html_part)
if generate_plots and image_buffers:
    for i, img_buffer in enumerate(image_buffers):
        img_buffer.seek(0)
        img = MIMEImage(img_buffer.read())
        img.add_header('Content-ID', f'<image{i}>')
        img.add_header('Content-Disposition', 'inline', filename=f'wykres_{i}.png')
        img_buffer.close()
        msg.attach(img)
try:
    with smtplib.SMTP(smtp_server) as server:
        server.sendmail(sender_email, receiver_email.split(','), msg.as_string())
    logging.info("E-mail został pomyślnie wysłany!")
except Exception:
    logging.exception("Wystąpił błąd podczas wysyłania e-maila.")

logging.info("--- Zakończono pracę skryptu ---")
